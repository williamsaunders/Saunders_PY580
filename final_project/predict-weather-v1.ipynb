{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520a53ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "from matplotlib_inline import backend_inline\n",
    "backend_inline.set_matplotlib_formats('retina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45974c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import torch # pytorch package, allows using GPUs\n",
    "# fix seed\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8a9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "mem_usage_start = mem_usage.used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b40747",
   "metadata": {},
   "source": [
    "# Step 1. Identify data and info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d3b4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file_info = pd.DataFrame({'filelist' : glob('data/tmax_train/*.h5')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6789c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file_info['num'] = [int(file.split('/')[2].split('_')[-1].split('.')[0]) for file in X_train_file_info['filelist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd23896",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file_info = X_train_file_info.sort_values('num')\n",
    "X_train_file_info.index = range(len(X_train_file_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "523e5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_info = np.load('data/tmax_train/tmax_X_train_info.npz').get('arr_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64575373",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load('data/tmax_train/tmax_y_train.npz', allow_pickle=True).get('arr_0')\n",
    "y_train = np.nan_to_num(y_train.astype(float), nan=-8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "683d1c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 6.4%\n",
      "Total: 251.65G\n",
      "Used: 15.19G\n",
      "Used - Start: 0.21G\n"
     ]
    }
   ],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa51ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_file_info = pd.DataFrame({'filelist' : glob('data/tmax_val/*.h5')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c661d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_file_info['num'] = [int(file.split('/')[2].split('_')[-1].split('.')[0]) for file in X_val_file_info['filelist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b0c46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_file_info = X_val_file_info.sort_values('num')\n",
    "X_val_file_info.index = range(len(X_val_file_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d6bfc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_info = np.load('data/tmax_val/tmax_X_val_info.npz').get('arr_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e550eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.load('data/tmax_val/tmax_y_val.npz', allow_pickle=True).get('arr_0')\n",
    "y_val = np.nan_to_num(y_val.astype(float), nan=-8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65455ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 6.4%\n",
      "Total: 251.65G\n",
      "Used: 15.19G\n",
      "Used - Start: 0.21G\n"
     ]
    }
   ],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de7839",
   "metadata": {},
   "source": [
    "# Step 2. Initialize the dataset with a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67b1c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, data_path, y_data, batch_size=1000):\n",
    "        super().__init__()\n",
    "        with h5py.File(data_path, 'r') as f:\n",
    "            self.data_X = np.array(f['data'])\n",
    "        self.data_y = np.array(np.split(y_data, len(self.data_X)//batch_size))\n",
    "        self.data_X = np.array(np.split(self.data_X, len(self.data_X)//batch_size))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_X[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_X)\n",
    "\n",
    "def load_data(dataset_path, y_data, i):\n",
    "    # define dataset path\n",
    "    \n",
    "    # create dataset object\n",
    "    batch_size = 1000\n",
    "    dataset = WeatherDataset(dataset_path[i], y_data[i*1000:i*1000+1000], batch_size=batch_size)\n",
    "\n",
    "    # create data loader object\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91005d53",
   "metadata": {},
   "source": [
    "data_chunk = load_data(X_train_file_info['filelist'].tolist(), y_train, 0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b182c58",
   "metadata": {},
   "source": [
    "data_chunk.dataset.data_X.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bc5a2a4",
   "metadata": {},
   "source": [
    "data_chunk.dataset.data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b499e359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 6.4%\n",
      "Total: 251.65G\n",
      "Used: 15.19G\n",
      "Used - Start: 0.21G\n"
     ]
    }
   ],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a560b",
   "metadata": {},
   "source": [
    "# Step 3. Design the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a56b8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PeriodicConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1):\n",
    "        super(PeriodicConv3d, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=0, dilation=dilation, groups=groups)\n",
    "\n",
    "        # Set the padding to 0 since we will add the padding manually using F.pad\n",
    "        self.conv.padding = (0, 0, 0)\n",
    "\n",
    "        # Save the kernel size and stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        # Compute the padding size\n",
    "        self.padding_size = ((kernel_size[0]-1)//2, (kernel_size[1]-1)//2, (kernel_size[2]-1)//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input tensor with the last slice of the tensor along each dimension\n",
    "        x = F.pad(x, (self.padding_size[2], self.padding_size[2], self.padding_size[1], self.padding_size[1], self.padding_size[0], self.padding_size[0]), mode='circular')\n",
    "\n",
    "        # Perform the convolution\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f468b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, batch_size=100, n_conv=2, out_channels=16, kernel_size=(5, 30, 5), stride=3, padding=5, \\\n",
    "                 fc_features=1024):\n",
    "        \"\"\"\n",
    "        batch_size is the batch size \n",
    "        n_conv is the number of convolutions to do\n",
    "        out_channels is the number of output channels from the first convolution. The second and onward double each time. \n",
    "        kernel_size is the shape of the convolution kernel\n",
    "        stride is the stride\n",
    "        fc_features is the number of features for the fully-connected layer\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # 3D convolutional layers\n",
    "        if padding == 'circular':\n",
    "            self.conv1 = PeriodicConv3d(in_channels=10, out_channels=out_channels, kernel_size=kernel_size, stride=stride, \\\n",
    "                                        padding=padding)\n",
    "            self.conv2 = PeriodicConv3d(in_channels=out_channels, out_channels=out_channels*2, kernel_size=kernel_size, \\\n",
    "                                        stride=stride, padding=padding)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv3d(in_channels=10, out_channels=out_channels, kernel_size=kernel_size, stride=stride, \\\n",
    "                                   padding=padding)\n",
    "            self.conv2 = nn.Conv3d(in_channels=out_channels, out_channels=out_channels*2, kernel_size=kernel_size, \\\n",
    "                                   stride=stride, padding=padding)\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm3d(num_features=out_channels)\n",
    "        self.bn2 = nn.BatchNorm3d(num_features=out_channels*2)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        if padding == 'circular':\n",
    "            num_features = np.array([\\\n",
    "                1, \\\n",
    "                out_channels*n_conv, \\\n",
    "                (( ((11+self.conv1.padding_size[0]*2 - kernel_size[0])//stride + 1) +self.conv1.padding_size[0]*2 - kernel_size[0] )//stride + 1),\\\n",
    "                (( ((365+self.conv1.padding_size[1]*2 - kernel_size[1])//stride + 1)//2 +self.conv1.padding_size[1]*2 - kernel_size[1] )//stride + 1)//2,\\\n",
    "                (( ((batch_size+self.conv1.padding_size[2]*2 - kernel_size[2])//stride + 1)//2 +self.conv1.padding_size[2]*2 - kernel_size[2] )//stride + 1)//2,\\\n",
    "                                    ]).astype(int)\n",
    "        else:\n",
    "            num_features = np.array([\\\n",
    "                1, \\\n",
    "                out_channels*n_conv, \\\n",
    "                (( ((11+padding*2 - kernel_size[0])//stride + 1) +padding*2 - kernel_size[0] )//stride + 1),\\\n",
    "                (( ((365+padding*2 - kernel_size[1])//stride + 1)//2 +padding*2 - kernel_size[1] )//stride + 1)//2,\\\n",
    "                (( ((batch_size+padding*2 - kernel_size[2])//stride + 1)//2 +padding*2 - kernel_size[2] )//stride + 1)//2,\\\n",
    "                                    ]).astype(int)\n",
    "        #print('----', num_features, np.prod(num_features))\n",
    "        self.fc1 = nn.Linear(in_features=np.prod(num_features), out_features=fc_features) #32*5*182*128, 32*11*91*250\n",
    "        self.fc2 = nn.Linear(in_features=fc_features, out_features=7*batch_size)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, batch_size):\n",
    "        # Input shape: (batch_size=400000, channels=10, depth=11, height=365, width=1)\n",
    "        \n",
    "        # First convolutional block\n",
    "        #print(x.shape)\n",
    "        # Perform the convolution\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.bn1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.bn2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Flatten\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Output shape: (batch_size=400000, num_classes=10)\n",
    "        return x.reshape((1, 7, batch_size))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80754d01",
   "metadata": {},
   "source": [
    "11 : (( ((11+padding*2 - kernel_size[0])//stride + 1) +padding*2 - kernel_size[0] )//stride + 1)\n",
    "365 : (( ((365+padding*2 - kernel_size[1])//stride + 1)//2 +padding*2 - kernel_size[1] )//stride + 1)//2\n",
    "batch_size : (( ((batch_size+padding*2 - kernel_size[2])//stride + 1)//2 +padding*2 - kernel_size[2] )//stride + 1)//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "916b9a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    \"A function that determines how close the outputs are to the real data\"\n",
    "    \"Report the total number within 5 and with 10 degrees celsius.\"\n",
    "    #print(output.shape)\n",
    "    #print(labels.shape)\n",
    "    \n",
    "    #rmse = torch.sqrt(torch.sum(labels - output)**2 / labels.size()[0])\n",
    "    within10 = torch.count_nonzero(torch.abs(labels - output) <= 100)\n",
    "    within5 = torch.count_nonzero(torch.abs(labels - output) <= 50)\n",
    "    \n",
    "    return np.array([within5.cpu(), within10.cpu()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ba59f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(predictions, labels):\n",
    "    loss = torch.pow(torch.abs(predictions - labels), 0.75)\n",
    "    loss = torch.median(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "027d27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_data, val_data, batch_size=100, num_epochs=10, loss_function='l1', test_mode=False, out=False):\n",
    "    # loss function can be `l1` or `custom` for now\n",
    "    # test_mode uses only 10 files for each, off by default\n",
    "    # out returns the statistics, off by default\n",
    "    # Set device to GPU if available, else CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    #criterion = nn.MSELoss()\n",
    "    if loss_function == 'l1':\n",
    "        criterion = nn.L1Loss(reduction='sum')\n",
    "    elif loss_function == 'custom':\n",
    "        criterion = custom_loss\n",
    "    else:\n",
    "        print('NO LOSS FUNCTION CHOSEN, BAD, BAD!')\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        \n",
    "        # loop over each file within the big dataset\n",
    "        # FOR NOW THIS IS GOING TO BE HARD-CODED AND I NEED TO ADJUST IT LATER\n",
    "        # there are 462 files, but let's not use the last one since it'll be a different shape, so 461 to use.\n",
    "        # in each epoch, select a random order to train them in \n",
    "        file_order = np.arange(0, 461)\n",
    "        if test_mode:\n",
    "            file_order = np.arange(0, 10)\n",
    "        np.random.shuffle(file_order)\n",
    "        \n",
    "        for file_i, file_num in enumerate(file_order):\n",
    "            print('Training on file', file_i+1, '/', len(file_order), '\\r', end='')\n",
    "            train_data = train_loader(file_i)\n",
    "            \n",
    "            num_batches = train_data.dataset.data_X.shape[1]//batch_size\n",
    "            # iterate over the batches\n",
    "            for batch_i in range(num_batches):\n",
    "                #print('batch', batch_i+1, '/', train_data.dataset.data_X.shape[1]//batch_size, '\\r', end='')\n",
    "                inputs = torch.Tensor(train_data.dataset.data_X[:, batch_i*batch_size:(batch_i+1)*batch_size, :, :, :]\\\n",
    "                                      .astype('int64')).reshape(1, 10, 11, 365, batch_size)\n",
    "                labels = torch.Tensor(train_data.dataset.data_y[:, batch_i*batch_size:(batch_i+1)*batch_size, :]\\\n",
    "                                      .astype('int64')).reshape(1, 7, batch_size)\n",
    "\n",
    "                # Move data to device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                #  pass\n",
    "                outputs = model(inputs, batch_size)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Track training loss and accuracy\n",
    "                train_loss += loss.item() * inputs.size(0)\n",
    "                #_, predicted = torch.max(outputs.data, 1)\n",
    "                predicted = outputs\n",
    "                train_correct += accuracy(predicted, labels)\n",
    "                \n",
    "                #del inputs, labels, outputs, loss\n",
    "                #gc.collect()\n",
    "                #torch.cuda.empty_cache()\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss /= len(train_data.dataset.data_X)\n",
    "        #train_accuracy = 100. * train_correct / len(train_data.dataset)\n",
    "        train_accuracy5 = 100*train_correct[0]/(predicted.numel()*num_batches*len(file_order))\n",
    "        train_accuracy10 = 100*train_correct[1]/(predicted.numel()*num_batches*len(file_order))\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "\n",
    "        # Loop through each validation file\n",
    "        val_file_order = np.arange(0, 46)\n",
    "        if test_mode:\n",
    "            val_file_order = np.arange(0, 10)\n",
    "        np.random.shuffle(val_file_order)\n",
    "        for file_i, file_num in enumerate(val_file_order):\n",
    "            print('Validating on file', file_i+1, '/', len(val_file_order), '\\r', end='')\n",
    "            val_data = val_loader(file_i)\n",
    "        \n",
    "            # Disable gradient computation\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # iterate over the batches for the validation set \n",
    "                for batch_i in range(val_data.dataset.data_X.shape[1]//batch_size):\n",
    "                    #print('batch', batch_i+1, '/', val_data.dataset.data_X.shape[1]//batch_size, '\\r', end='')\n",
    "                    inputs = torch.Tensor(val_data.dataset.data_X[:, batch_i*batch_size:(batch_i+1)*batch_size, :, :, :]\\\n",
    "                                          .astype('int64')).reshape(1, 10, 11, 365, batch_size)\n",
    "                    labels = torch.Tensor(val_data.dataset.data_y[:, batch_i*batch_size:(batch_i+1)*batch_size, :]\\\n",
    "                                          .astype('int64')).reshape(1, 7, batch_size)\n",
    "                    \n",
    "                    # Move data to device\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    #  pass\n",
    "                    outputs = model(inputs, batch_size)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Track validation loss and accuracy\n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "                    #_, predicted = torch.max(outputs.data, 1)\n",
    "                    predicted = outputs\n",
    "                    val_correct += accuracy(predicted, labels)\n",
    "                    \n",
    "                    #del inputs, labels, outputs, loss\n",
    "                    #gc.collect()\n",
    "                    #torch.cuda.empty_cache()\n",
    "            \n",
    "        # Calculate average validation loss and accuracy\n",
    "        val_loss /= len(val_data.dataset.data_X)\n",
    "        #val_accuracy = 100. * val_correct / len(val_data.dataset)\n",
    "        val_accuracy5 = 100*val_correct[0]/(predicted.numel()*num_batches*len(val_file_order))\n",
    "        val_accuracy10 = 100*val_correct[1]/(predicted.numel()*num_batches*len(val_file_order))\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(\"Epoch [{}/{}] Train Loss: {:.2e}, Train Acc10: {:.1f}%, Train Acc5: {:.1f}% | \\\n",
    "Val Loss: {:.2e}, Val Acc10: {:.1f}%, Val Acc5: {:.1f}%\"\n",
    "              .format(epoch+1, num_epochs, train_loss, train_accuracy10, train_accuracy5, \\\n",
    "                      val_loss, val_accuracy10, val_accuracy5))\n",
    "\n",
    "    if out:\n",
    "        return (train_loss, train_accuracy10, train_accuracy5, val_loss, val_accuracy10, val_accuracy5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1aa12cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loader(i):\n",
    "    return load_data(X_train_file_info['filelist'].tolist(), y_train, i)\n",
    "\n",
    "def val_loader(i):\n",
    "    return load_data(X_val_file_info['filelist'].tolist(), y_val, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149eaf4c",
   "metadata": {},
   "source": [
    "# Step 4. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b52c46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    del(model)\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73079ea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(batch_size=25, n_conv=2, out_channels=64, kernel_size=(5, 30, 5), stride=2, padding='circular', fc_features=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5c4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on file 5 / 10 \r"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, num_epochs=100, batch_size=25, test_mode=True, loss_function='custom')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecddb63",
   "metadata": {},
   "source": [
    "# Step 5. Tune the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26468549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let stride = 2\n",
    "kernel_size_list = [\n",
    "[3, 10, 3],\n",
    "[3, 30, 3],\n",
    "[3, 50, 3],\n",
    "\n",
    "[5, 10, 5],\n",
    "[5, 30, 5],\n",
    "[5, 50, 5]]\n",
    "out_channels_list = [32]\n",
    "fc_features_list = [1024, 2048]\n",
    "padding_list = ['circular']\n",
    "loss_function_list = ['l1', 'custom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cba479ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6 * 1 * 2 * 1 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de92ab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, kernel_size in enumerate(kernel_size_list):\n",
    "    for j, out_channels in enumerate(out_channels_list):\n",
    "        for k, fc_features in enumerate(fc_features_list):\n",
    "            for l, padding in enumerate(padding_list):\n",
    "                for m, loss_function in enumerate(loss_function_list):\n",
    "                    #print((i+1)*(j+1)*(k+1), '/', len(kernel_size_list)*len(out_channels_list)*len(fc_features_list), '\\r', end='')\n",
    "                    print('\\nkernel_size :', kernel_size, '| out_channels :', out_channels, ' | fc_features :', fc_features, \\\n",
    "                          '| loss func :', loss_function)\n",
    "                    try: \n",
    "                        del(model)\n",
    "                    except NameError:\n",
    "                        pass\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    model = Model(batch_size=25, n_conv=2, out_channels=out_channels, kernel_size=kernel_size, stride=2, padding=1,\\\n",
    "                                  fc_features=fc_features)\n",
    "\n",
    "                    out = train(model, train_loader, val_loader, num_epochs=10, batch_size=25, out=True, test_mode=True, \\\n",
    "                                loss_function=loss_function)\n",
    "                    with open('hyperparameter_results.txt', 'a') as f:\n",
    "                        f.write(\"Train Loss: {:.2e}, Train Acc10: {:.1f}%, Train Acc5: {:.1f}% | \\\n",
    "    Val Loss: {:.2e}, Val Acc10: {:.1f}%, Val Acc5: {:.1f}%\\n\".format(out[0], out[1], out[2], out[3], out[4], out[5]))\n",
    "                    f.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4ef877d",
   "metadata": {},
   "source": [
    "Top 5 performing on validation, 5 degrees: \n",
    "\n",
    "[5, 10, 5], 32, 2048, l1\n",
    "[5, 10, 5], 32, 1024, l1\n",
    "[3, 10, 3], 32, 2048, custom\n",
    "[3, 30, 3], 32, 1024, custom\n",
    "[5, 10, 5], 32, 1024, custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8ea87f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 2.4%\n",
      "Total: 376.36G\n",
      "Used: 7.60G\n",
      "Used - Start: 2.39G\n"
     ]
    }
   ],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eff61b",
   "metadata": {},
   "source": [
    "# Step 6. Run [what I think might be] the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7277be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Train Loss: 5.26e+05, Train Acc10: 86.2%, Train Acc5: 57.0% | Val Loss: 1.75e+05, Val Acc10: 52.6%, Val Acc5: 27.0%\n",
      "Epoch [2/5] Train Loss: 5.22e+05, Train Acc10: 86.7%, Train Acc5: 57.9% | Val Loss: 1.74e+05, Val Acc10: 53.7%, Val Acc5: 28.3%\n",
      "Epoch [3/5] Train Loss: nan, Train Acc10: 2.8%, Train Acc5: 1.9% | Val Loss: nan, Val Acc10: 0.0%, Val Acc5: 0.0%\n",
      "Epoch [4/5] Train Loss: nan, Train Acc10: 0.0%, Train Acc5: 0.0% | Val Loss: nan, Val Acc10: 0.0%, Val Acc5: 0.0%\n",
      "Epoch [5/5] Train Loss: nan, Train Acc10: 0.0%, Train Acc5: 0.0% | Val Loss: nan, Val Acc10: 0.0%, Val Acc5: 0.0%\n"
     ]
    }
   ],
   "source": [
    "model = Model(batch_size=25, n_conv=2, out_channels=64, kernel_size=(5, 30, 5), stride=2, padding='circular', fc_features=2048)\n",
    "\n",
    "train(model, train_loader, val_loader, num_epochs=5, batch_size=25, test_mode=False, loss_function='custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c2834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] Train Loss: 4.97e+08, Train Acc10: 86.6%, Train Acc5: 57.4% | Val Loss: 2.44e+08, Val Acc10: 49.9%, Val Acc5: 25.4%\n",
      "Epoch [2/50] Train Loss: 4.96e+08, Train Acc10: 86.9%, Train Acc5: 58.0% | Val Loss: 2.41e+08, Val Acc10: 54.6%, Val Acc5: 29.5%\n",
      "Epoch [3/50] Train Loss: 4.96e+08, Train Acc10: 86.9%, Train Acc5: 58.1% | Val Loss: 2.39e+08, Val Acc10: 58.0%, Val Acc5: 32.1%\n",
      "Epoch [4/50] Train Loss: 4.96e+08, Train Acc10: 86.8%, Train Acc5: 58.0% | Val Loss: 2.38e+08, Val Acc10: 58.8%, Val Acc5: 32.2%\n",
      "Epoch [5/50] Train Loss: 4.88e+08, Train Acc10: 88.2%, Train Acc5: 59.9% | Val Loss: 2.36e+08, Val Acc10: 61.1%, Val Acc5: 33.6%\n",
      "Epoch [6/50] Train Loss: 4.83e+08, Train Acc10: 89.0%, Train Acc5: 61.0% | Val Loss: 2.35e+08, Val Acc10: 60.7%, Val Acc5: 33.4%\n",
      "Epoch [7/50] Train Loss: 4.82e+08, Train Acc10: 89.2%, Train Acc5: 61.2% | Val Loss: 2.34e+08, Val Acc10: 63.4%, Val Acc5: 35.1%\n",
      "Epoch [8/50] Train Loss: 4.80e+08, Train Acc10: 89.4%, Train Acc5: 61.5% | Val Loss: 2.31e+08, Val Acc10: 65.9%, Val Acc5: 37.0%\n",
      "Epoch [9/50] Train Loss: 4.78e+08, Train Acc10: 89.5%, Train Acc5: 61.6% | Val Loss: 2.30e+08, Val Acc10: 66.3%, Val Acc5: 37.1%\n",
      "Epoch [10/50] Train Loss: 4.73e+08, Train Acc10: 89.4%, Train Acc5: 61.6% | Val Loss: 2.24e+08, Val Acc10: 69.2%, Val Acc5: 39.2%\n",
      "Epoch [11/50] Train Loss: 4.19e+08, Train Acc10: 88.9%, Train Acc5: 60.8% | Val Loss: 1.42e+08, Val Acc10: 57.4%, Val Acc5: 30.7%\n",
      "Epoch [12/50] Train Loss: 3.32e+08, Train Acc10: 88.9%, Train Acc5: 60.8% | Val Loss: 1.46e+08, Val Acc10: 45.1%, Val Acc5: 23.1%\n",
      "Epoch [13/50] Train Loss: 3.29e+08, Train Acc10: 89.1%, Train Acc5: 61.0% | Val Loss: 1.49e+08, Val Acc10: 40.8%, Val Acc5: 21.2%\n",
      "Epoch [14/50] Train Loss: 3.23e+08, Train Acc10: 89.1%, Train Acc5: 61.1% | Val Loss: 1.54e+08, Val Acc10: 39.9%, Val Acc5: 20.5%\n",
      "Epoch [15/50] Train Loss: 3.17e+08, Train Acc10: 89.1%, Train Acc5: 61.2% | Val Loss: 1.48e+08, Val Acc10: 39.1%, Val Acc5: 20.2%\n",
      "Epoch [16/50] Train Loss: 3.10e+08, Train Acc10: 89.1%, Train Acc5: 61.2% | Val Loss: 1.54e+08, Val Acc10: 36.3%, Val Acc5: 18.9%\n",
      "Epoch [17/50] Train Loss: 3.09e+08, Train Acc10: 89.2%, Train Acc5: 61.2% | Val Loss: 1.52e+08, Val Acc10: 39.6%, Val Acc5: 20.6%\n",
      "Epoch [18/50] Train Loss: 3.02e+08, Train Acc10: 89.3%, Train Acc5: 61.3% | Val Loss: 1.57e+08, Val Acc10: 37.0%, Val Acc5: 19.3%\n",
      "Epoch [19/50] Train Loss: 2.99e+08, Train Acc10: 89.3%, Train Acc5: 61.4% | Val Loss: 1.57e+08, Val Acc10: 37.5%, Val Acc5: 19.4%\n",
      "Epoch [20/50] Train Loss: 2.98e+08, Train Acc10: 89.3%, Train Acc5: 61.4% | Val Loss: 1.59e+08, Val Acc10: 34.9%, Val Acc5: 18.3%\n",
      "Epoch [21/50] Train Loss: 2.96e+08, Train Acc10: 89.3%, Train Acc5: 61.3% | Val Loss: 1.66e+08, Val Acc10: 38.9%, Val Acc5: 20.2%\n",
      "Epoch [22/50] Train Loss: 2.93e+08, Train Acc10: 89.3%, Train Acc5: 61.4% | Val Loss: 1.60e+08, Val Acc10: 38.5%, Val Acc5: 20.1%\n",
      "Epoch [23/50] Train Loss: 2.91e+08, Train Acc10: 89.4%, Train Acc5: 61.6% | Val Loss: 1.67e+08, Val Acc10: 36.0%, Val Acc5: 18.8%\n",
      "Epoch [24/50] Train Loss: 2.93e+08, Train Acc10: 89.3%, Train Acc5: 61.5% | Val Loss: 1.73e+08, Val Acc10: 36.1%, Val Acc5: 18.9%\n",
      "Epoch [25/50] Train Loss: 2.93e+08, Train Acc10: 89.4%, Train Acc5: 61.6% | Val Loss: 1.70e+08, Val Acc10: 35.7%, Val Acc5: 18.6%\n",
      "Epoch [26/50] Train Loss: 2.90e+08, Train Acc10: 89.5%, Train Acc5: 61.6% | Val Loss: 1.84e+08, Val Acc10: 33.5%, Val Acc5: 17.6%\n",
      "Epoch [27/50] Train Loss: 2.90e+08, Train Acc10: 89.6%, Train Acc5: 61.8% | Val Loss: 1.97e+08, Val Acc10: 32.7%, Val Acc5: 17.1%\n",
      "Epoch [28/50] Train Loss: 2.89e+08, Train Acc10: 89.5%, Train Acc5: 61.7% | Val Loss: 1.78e+08, Val Acc10: 36.5%, Val Acc5: 18.8%\n",
      "Epoch [29/50] Train Loss: 2.89e+08, Train Acc10: 89.6%, Train Acc5: 61.8% | Val Loss: 1.99e+08, Val Acc10: 35.3%, Val Acc5: 18.5%\n",
      "Epoch [30/50] Train Loss: 2.90e+08, Train Acc10: 89.6%, Train Acc5: 61.8% | Val Loss: 1.98e+08, Val Acc10: 38.3%, Val Acc5: 20.2%\n",
      "Epoch [31/50] Train Loss: 2.86e+08, Train Acc10: 89.7%, Train Acc5: 62.0% | Val Loss: 1.81e+08, Val Acc10: 35.3%, Val Acc5: 18.5%\n",
      "Epoch [32/50] Train Loss: 2.86e+08, Train Acc10: 89.7%, Train Acc5: 61.9% | Val Loss: 2.04e+08, Val Acc10: 37.7%, Val Acc5: 19.5%\n",
      "Epoch [33/50] Train Loss: 2.87e+08, Train Acc10: 89.7%, Train Acc5: 62.1% | Val Loss: 2.27e+08, Val Acc10: 36.1%, Val Acc5: 18.7%\n",
      "Epoch [34/50] Train Loss: 2.85e+08, Train Acc10: 89.7%, Train Acc5: 62.1% | Val Loss: 1.90e+08, Val Acc10: 42.3%, Val Acc5: 22.1%\n",
      "Epoch [35/50] Train Loss: 2.85e+08, Train Acc10: 89.8%, Train Acc5: 62.2% | Val Loss: 2.17e+08, Val Acc10: 39.3%, Val Acc5: 20.5%\n",
      "Epoch [36/50] Train Loss: 2.84e+08, Train Acc10: 89.8%, Train Acc5: 62.2% | Val Loss: 2.15e+08, Val Acc10: 35.5%, Val Acc5: 18.5%\n",
      "Epoch [37/50] Train Loss: 2.83e+08, Train Acc10: 89.9%, Train Acc5: 62.4% | Val Loss: 1.89e+08, Val Acc10: 41.0%, Val Acc5: 21.1%\n",
      "Epoch [38/50] Train Loss: 2.82e+08, Train Acc10: 89.9%, Train Acc5: 62.4% | Val Loss: 2.27e+08, Val Acc10: 36.7%, Val Acc5: 19.1%\n",
      "Training on file 397 / 461 \r"
     ]
    }
   ],
   "source": [
    "model = Model(batch_size=25, n_conv=2, out_channels=64, kernel_size=(5, 30, 5), stride=2, padding='circular', fc_features=2048)\n",
    "\n",
    "train(model, train_loader, val_loader, num_epochs=50, batch_size=25, test_mode=False, loss_function='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0b98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
