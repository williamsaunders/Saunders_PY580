{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520a53ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "from matplotlib_inline import backend_inline\n",
    "backend_inline.set_matplotlib_formats('retina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45974c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import torch # pytorch package, allows using GPUs\n",
    "# fix seed\n",
    "seed=17\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8a9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "mem_usage_start = mem_usage.used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b40747",
   "metadata": {},
   "source": [
    "# Step 1. Identify data and info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d3b4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file_info = pd.DataFrame({'filelist' : glob('data/tmax_train/*.h5')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6789c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file_info['num'] = [int(file.split('/')[2].split('_')[-1].split('.')[0]) for file in X_train_file_info['filelist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd23896",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_file_info = X_train_file_info.sort_values('num')\n",
    "X_train_file_info.index = range(len(X_train_file_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "523e5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_info = np.load('data/tmax_train/tmax_X_train_info.npz').get('arr_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64575373",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load('data/tmax_train/tmax_y_train.npz', allow_pickle=True).get('arr_0')\n",
    "y_train = np.nan_to_num(y_train.astype(float), nan=-8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "683d1c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 3.6%\n",
      "Total: 251.65G\n",
      "Used: 7.99G\n",
      "Used - Start: 0.21G\n"
     ]
    }
   ],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa51ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_file_info = pd.DataFrame({'filelist' : glob('data/tmax_val/*.h5')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c661d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_file_info['num'] = [int(file.split('/')[2].split('_')[-1].split('.')[0]) for file in X_val_file_info['filelist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b0c46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_file_info = X_val_file_info.sort_values('num')\n",
    "X_val_file_info.index = range(len(X_val_file_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d6bfc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_info = np.load('data/tmax_val/tmax_X_val_info.npz').get('arr_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e550eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.load('data/tmax_val/tmax_y_val.npz', allow_pickle=True).get('arr_0')\n",
    "y_val = np.nan_to_num(y_val.astype(float), nan=-8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65455ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 3.6%\n",
      "Total: 251.65G\n",
      "Used: 7.99G\n",
      "Used - Start: 0.21G\n"
     ]
    }
   ],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de7839",
   "metadata": {},
   "source": [
    "# Step 2. Initialize the dataset with a data loader"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6883bb5a",
   "metadata": {},
   "source": [
    "from torchvision import datasets # load data\n",
    "\n",
    "class Weather_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Weather pytorch dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_type, transform=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_type (string): `train`, `validate` or `test`: creates data_loader\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #from sklearn.model_selection import train_test_split\n",
    "        #import collections\n",
    "        #import pickle as pickle\n",
    "\n",
    "        # path to data directory\n",
    "        train_data_filelist = X_train_file_info['filelist'].tolist()\n",
    "\n",
    "    '''\n",
    "    # override __len__ and __getitem__ of the Dataset() class\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data[1])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample=(self.data[0][idx,...],self.data[1][idx])\n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    '''\n",
    "    \n",
    "def load_data(kwargs):\n",
    "    # kwargs:  CUDA arguments, if enabled\n",
    "    # load and noralise train,test, and data\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        Ising_Dataset(data_type='train'),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        Ising_Dataset(data_type='test'),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    critical_loader = torch.utils.data.DataLoader(\n",
    "        Ising_Dataset(data_type='critical'),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader, critical_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67b1c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, data_path, y_data, batch_size=1000):\n",
    "        super().__init__()\n",
    "        with h5py.File(data_path, 'r') as f:\n",
    "            self.data_X = np.array(f['data'])\n",
    "        self.data_y = np.array(np.split(y_data, len(self.data_X)//batch_size))\n",
    "        self.data_X = np.array(np.split(self.data_X, len(self.data_X)//batch_size))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_X[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_X)\n",
    "\n",
    "def load_data(dataset_path, y_data, i):\n",
    "    # define dataset path\n",
    "    \n",
    "    # create dataset object\n",
    "    batch_size = 1000\n",
    "    dataset = WeatherDataset(dataset_path[i], y_data[i*1000:i*1000+1000], batch_size=batch_size)\n",
    "\n",
    "    # create data loader object\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26e8af96",
   "metadata": {},
   "source": [
    "data_chunk = load_data(X_train_file_info['filelist'].tolist(), y_train, 0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb525f22",
   "metadata": {},
   "source": [
    "data_chunk.dataset.data_X.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03b6e7a7",
   "metadata": {},
   "source": [
    "data_chunk.dataset.data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b499e359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 5.0%\n",
      "Total: 251.65G\n",
      "Used: 11.48G\n",
      "Used - Start: 3.70G\n"
     ]
    }
   ],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "59dfdc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10 # NEED TO FIX THIS LATER, INTEGRATE BETTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "9f468b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # construct NN\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, batch_size=10):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # 3D convolutional layers\n",
    "        self.conv1 = nn.Conv3d(in_channels=10, out_channels=16, kernel_size=(5, 30, 5), stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(in_channels=16, out_channels=32, kernel_size=(5, 30, 5), stride=1, padding=1)\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm3d(num_features=16)\n",
    "        self.bn2 = nn.BatchNorm3d(num_features=32)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=15904, out_features=1024) #32*5*182*128, 32*11*91*250\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=7*batch_size)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size=400000, channels=10, depth=11, height=365, width=1)\n",
    "        \n",
    "        # First convolutional block\n",
    "        #print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Flatten\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Output shape: (batch_size=400000, num_classes=10)\n",
    "        return x.reshape((1, 7, batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "916b9a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    \"A function that determines how close the outputs are to the real data\"\n",
    "    \"Consider accurate if temperature within 10 degrees\"\n",
    "    #print(output.shape)\n",
    "    #print(labels.shape)\n",
    "    \n",
    "    #rmse = torch.sqrt(torch.sum(labels - output)**2 / labels.size()[0])\n",
    "    within10 = torch.count_nonzero(torch.abs(labels - output) <= 10)\n",
    "    \n",
    "    return within10"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de69c595",
   "metadata": {},
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_loader, val_loader, batch_size=100, num_epochs=10, learning_rate=0.001):\n",
    "    # Set device to GPU if available, else CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        \n",
    "        num_batches = train_loader.dataset.data_X.shape[1]//batch_size\n",
    "        # iterate over the batches\n",
    "        for batch_i in range(num_batches):\n",
    "            print('batch', batch_i+1, '/', train_loader.dataset.data_X.shape[1]//batch_size, '\\r', end='')\n",
    "            inputs = torch.Tensor(train_loader.dataset.data_X[:, batch_i*batch_size:(batch_i+1)*batch_size, :, :, :]\\\n",
    "                                  .astype('int64')).reshape(1, 10, 11, 365, batch_size)\n",
    "            labels = torch.Tensor(train_loader.dataset.data_y[:, batch_i*batch_size:(batch_i+1)*batch_size, :]\\\n",
    "                                  .astype('int64')).reshape(1, 7, batch_size)\n",
    "            \n",
    "            # Move data to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track training loss and accuracy\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = outputs\n",
    "            train_correct += accuracy(predicted, labels)\n",
    "        \n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss /= len(train_loader.dataset.data_X)\n",
    "        #train_accuracy = 100. * train_correct / len(train_loader.dataset)\n",
    "        train_accuracy = 100*train_correct/(predicted.numel()*num_batches) #torch.sum(train_correct)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        \n",
    "        # Disable gradient computation\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # iterate over the batches for the validation set \n",
    "            for batch_i in range(val_loader.dataset.data_X.shape[1]//batch_size):\n",
    "                print('batch', batch_i+1, '/', val_loader.dataset.data_X.shape[1]//batch_size, '\\r', end='')\n",
    "                inputs = torch.Tensor(val_loader.dataset.data_X[:, batch_i*batch_size:(batch_i+1)*batch_size, :, :, :]\\\n",
    "                                      .astype('int64')).reshape(1, 10, 11, 365, batch_size)\n",
    "                labels = torch.Tensor(val_loader.dataset.data_y[:, batch_i*batch_size:(batch_i+1)*batch_size, :]\\\n",
    "                                      .astype('int64')).reshape(1, 7, batch_size)\n",
    "                # Move data to device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Track validation loss and accuracy\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                #_, predicted = torch.max(outputs.data, 1)\n",
    "                predicted = outputs\n",
    "                val_correct += accuracy(predicted, labels)\n",
    "            \n",
    "        # Calculate average validation loss and accuracy\n",
    "        val_loss /= len(val_loader.dataset.data_X)\n",
    "        #val_accuracy = 100. * val_correct / len(val_loader.dataset)\n",
    "        val_accuracy = 100*val_correct/(predicted.numel()*num_batches)  #torch.sum(val_correct)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(\"Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.2f}%, Val Loss: {:.4f}, Val Acc: {:.2f}%\"\n",
    "              .format(epoch+1, num_epochs, train_loss, train_accuracy, val_loss, val_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "f8cc46e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_data, val_data, batch_size=100, num_epochs=10, learning_rate=0.001):\n",
    "    # Set device to GPU if available, else CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        \n",
    "        # loop over each file within the big dataset\n",
    "        # FOR NOW THIS IS GOING TO BE HARD-CODED AND I NEED TO ADJUST IT LATER\n",
    "        # there are 462 files, but let's not use the last one since it'll be a different shape, so 461 to use.\n",
    "        # in each epoch, select a random order to train them in \n",
    "        file_order = np.arange(0, 461)\n",
    "        np.random.shuffle(file_order)\n",
    "        \n",
    "        for file_i, file_num in enumerate(file_order):\n",
    "            print('Training on file', file_i+1, '/', len(file_order), '\\r', end='')\n",
    "            train_data = train_loader(file_i)\n",
    "            \n",
    "            num_batches = train_data.dataset.data_X.shape[1]//batch_size\n",
    "            # iterate over the batches\n",
    "            for batch_i in range(num_batches):\n",
    "                #print('batch', batch_i+1, '/', train_data.dataset.data_X.shape[1]//batch_size, '\\r', end='')\n",
    "                inputs = torch.Tensor(train_data.dataset.data_X[:, batch_i*batch_size:(batch_i+1)*batch_size, :, :, :]\\\n",
    "                                      .astype('int64')).reshape(1, 10, 11, 365, batch_size)\n",
    "                labels = torch.Tensor(train_data.dataset.data_y[:, batch_i*batch_size:(batch_i+1)*batch_size, :]\\\n",
    "                                      .astype('int64')).reshape(1, 7, batch_size)\n",
    "\n",
    "                # Move data to device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Track training loss and accuracy\n",
    "                train_loss += loss.item() * inputs.size(0)\n",
    "                #_, predicted = torch.max(outputs.data, 1)\n",
    "                predicted = outputs\n",
    "                train_correct += accuracy(predicted, labels)\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss /= len(train_data.dataset.data_X)\n",
    "        #train_accuracy = 100. * train_correct / len(train_data.dataset)\n",
    "        train_accuracy = 100*train_correct/(predicted.numel()*num_batches*len(file_order)) #torch.sum(train_correct)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "\n",
    "        # Loop through each validation file\n",
    "        val_file_order = np.arange(0, 46)\n",
    "        np.random.shuffle(val_file_order)\n",
    "        for file_i, file_num in enumerate(val_file_order):\n",
    "            print('Validating on file', file_i+1, '/', len(val_file_order), '\\r', end='')\n",
    "            val_data = val_loader(file_i)\n",
    "        \n",
    "            # Disable gradient computation\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # iterate over the batches for the validation set \n",
    "                for batch_i in range(val_data.dataset.data_X.shape[1]//batch_size):\n",
    "                    #print('batch', batch_i+1, '/', val_data.dataset.data_X.shape[1]//batch_size, '\\r', end='')\n",
    "                    inputs = torch.Tensor(val_data.dataset.data_X[:, batch_i*batch_size:(batch_i+1)*batch_size, :, :, :]\\\n",
    "                                          .astype('int64')).reshape(1, 10, 11, 365, batch_size)\n",
    "                    labels = torch.Tensor(val_data.dataset.data_y[:, batch_i*batch_size:(batch_i+1)*batch_size, :]\\\n",
    "                                          .astype('int64')).reshape(1, 7, batch_size)\n",
    "                    # Move data to device\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Track validation loss and accuracy\n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "                    #_, predicted = torch.max(outputs.data, 1)\n",
    "                    predicted = outputs\n",
    "                    val_correct += accuracy(predicted, labels)\n",
    "            \n",
    "        # Calculate average validation loss and accuracy\n",
    "        val_loss /= len(val_data.dataset.data_X)\n",
    "        #val_accuracy = 100. * val_correct / len(val_data.dataset)\n",
    "        val_accuracy = 100*val_correct/(predicted.numel()*num_batches*len(val_file_order))  #torch.sum(val_correct)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(\"Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.2f}%, Val Loss: {:.4f}, Val Acc: {:.2f}%\"\n",
    "              .format(epoch+1, num_epochs, train_loss, train_accuracy, val_loss, val_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "a1679f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loader(i):\n",
    "    return load_data(X_train_file_info['filelist'].tolist(), y_train, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "7e1ba16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loader(i):\n",
    "    return load_data(X_val_file_info['filelist'].tolist(), y_val, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "73079ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "a4e908e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 4.3%\n",
      "Total: 251.65G\n",
      "Used: 10.01G\n",
      "Used - Start: 2.23G\n"
     ]
    }
   ],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5c4e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 43038656667.7741, Train Acc: 7.18%, Val Loss: 28438448288.5553, Val Acc: 3.05%\n",
      "Epoch [2/5], Train Loss: 42728896512.1590, Train Acc: 8.14%, Val Loss: 27621158441.7784, Val Acc: 1.86%\n",
      "Epoch [3/5], Train Loss: 22564004455.0528, Train Acc: 9.04%, Val Loss: 14820619439.6812, Val Acc: 1.79%\n",
      "Epoch [4/5], Train Loss: 16868909290.3411, Train Acc: 9.89%, Val Loss: 10738533269.5609, Val Acc: 2.15%\n",
      "Training on file 8 / 461 \r"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, num_epochs=5, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage = psutil.virtual_memory()\n",
    "\n",
    "print(f\"Free: {mem_usage.percent}%\")\n",
    "print(f\"Total: {mem_usage.total/(1024**3):.2f}G\")\n",
    "print(f\"Used: {mem_usage.used/(1024**3):.2f}G\")\n",
    "print(f\"Used - Start: {(mem_usage.used - mem_usage_start)/(1024**3):.2f}G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7277be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d3258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a8095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745b4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c02b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3766d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7458c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # construct NN\n",
    "\n",
    "class model(nn.Module):\n",
    "    # create convolutional net\n",
    "    def __init__(self, N=10, L=40):\n",
    "        # inherit attributes and methods of nn.Module\n",
    "        super(model, self).__init__()\t\n",
    "        # create convolutional layer with input depth 1 and output depth N\n",
    "        self.conv1 = nn.Conv3d(10, N, kernel_size=3, padding=1)\n",
    "        # batch norm layer takes Depth\n",
    "        self.bn1=nn.BatchNorm3d(N) \n",
    "        # create fully connected layer after maxpool operation reduced 40->18\n",
    "        self.fc1 = nn.Linear(1000, 7) \t\n",
    "        self.N=N\n",
    "        self.L=L\n",
    "        print(\"The number of neurons in CNN layer is %i\"%(N))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.Tensor(x)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)  often useful to look at shapes for debugging\n",
    "        x = F.max_pool3d(x,3)\n",
    "        #print(x.shape)\n",
    "        x=self.bn1(x) # largely unnecessary and here just for pedagogical purposes\n",
    "        return F.log_softmax(self.fc1(x.view(-1,20*20*self.N)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3e4930e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, epoch):\n",
    "    CNN.train() # effects Dropout and BatchNorm layers\n",
    "    data = train_loader.dataset.data_X_train\n",
    "    target = train_loader.dataset.data_y_train\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = CNN(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % args.log_interval == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, 0 * len(data), len(train_loader.dataset),\n",
    "            100. * 0 / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ae9301d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_loader,verbose='Test'):\n",
    "    # these are very standard functions for evaluating data\n",
    "\n",
    "    CNN.eval() # effects Dropout and BatchNorm layers\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in data_loader:\n",
    "        output = CNN(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    print('\\n'+verbose+' set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))\n",
    "    accuracy=100. * correct / len(data_loader.dataset)\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "05f6ea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of neurons in CNN layer is 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [1, 10, 3, 3, 3], expected input[1, 1000, 10, 11, 365] to have 10 channels, but got 1000 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/scratch/7493164.1.biophys-gpu-pub/ipykernel_25423/2882857495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#optimizer = optim.Adam(DNN.parameters(), lr=0.001, betas=(0.9, 0.999))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/7493164.1.biophys-gpu-pub/ipykernel_25423/2780329421.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_y_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/7493164.1.biophys-gpu-pub/ipykernel_25423/1927643158.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print(x.shape)  often useful to look at shapes for debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.13.1/install/lib/SCC/../python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             )\n\u001b[0;32m--> 608\u001b[0;31m         return F.conv3d(\n\u001b[0m\u001b[1;32m    609\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [1, 10, 3, 3, 3], expected input[1, 1000, 10, 11, 365] to have 10 channels, but got 1000 channels instead"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F # implements forward and backward definitions of an autograd operation\n",
    "import torch.optim as optim # different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc\n",
    "\n",
    "CNN = model(N=1)\n",
    "# negative log-likelihood (nll) loss for training: takes class labels NOT one-hot vectors!\n",
    "criterion = F.nll_loss\n",
    "# define optimizer\n",
    "optimizer = optim.SGD(CNN.parameters(), lr=0.001, momentum=0.001)\n",
    "#optimizer = optim.Adam(DNN.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "train(train_loader(0), 1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e94bc9d9",
   "metadata": {},
   "source": [
    "import argparse # handles arguments\n",
    "import sys; sys.argv=['']; del sys # required to use parser in jupyter notebooks\n",
    "\n",
    "# training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Convmodel Ising Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "args.epochs=5\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "cuda_kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e7f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader, critical_loader=load_data(cuda_kwargs)\n",
    "\n",
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dcf15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b43e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array=[]\n",
    "critical_array=[]\n",
    "\n",
    "# create array of depth of convolutional layer\n",
    "N_array=[1]\n",
    "\n",
    "# loop over depths\n",
    "for N in N_array:\n",
    "    CNN = model(N=N)\n",
    "\n",
    "    # negative log-likelihood (nll) loss for training: takes class labels NOT one-hot vectors!\n",
    "    criterion = F.nll_loss\n",
    "    # define optimizer\n",
    "    #optimizer = optim.SGD(CNN.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    optimizer = optim.Adam(DNN.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "    # train the CNN and test its performance at each epoch\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(epoch)\n",
    "        if epoch==args.epochs:\n",
    "            test_array.append(test(test_loader,verbose='Test'))\n",
    "        else:\n",
    "            test(test_loader,verbose='Test')\n",
    "    print(test_array)\n",
    "    print(critical_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
